#!/usr/bin/env python3
"""
è½»é‡çº§ Gemini API å…¼å®¹ä»£ç†æœåŠ¡å™¨
æ¥æ”¶ Gemini æ ¼å¼è¯·æ±‚ï¼Œè½¬å‘ç»™ OpenAI æˆ–å…¶ä»– OPNAIæ ‡å‡†APIçš„LLMï¼Œè¿”å› Gemini æ ¼å¼å“åº”
"""

import json
import uuid
import datetime
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
import logging
import os

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆç¨åä¼šæ ¹æ®é…ç½®è®¾ç½®ï¼‰
request_logger = None
access_logger = None

# åŠ è½½é…ç½®æ–‡ä»¶
def load_config():
    """ä» config.json åŠ è½½é…ç½®"""
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except FileNotFoundError:
        logger.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "openai_api_key": "your-api-key",
            "openai_base_url": "https://api.openai.com/v1",
            "model_mapping": {
                "gemini-2.5-pro": "gpt-4",
                "gemini-2.5-flash": "gpt-3.5-turbo"
            },
            "default_openai_model": "gpt-3.5-turbo",
            "server": {
                "host": "0.0.0.0",
                "port": 8000,
                "log_level": "info"
            },
            "logging": {
                "enable_detailed_logs": false,
                "enable_access_logs": true,
                "log_directory": "logs"
            }
        }
    except json.JSONDecodeError as e:
        logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        raise

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
def init_logging_system(config):
    """æ ¹æ®é…ç½®åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
    global request_logger, access_logger
    
    logging_config = config.get("logging", {})
    log_dir = logging_config.get("log_directory", "logs")
    enable_detailed = logging_config.get("enable_detailed_logs", False)
    enable_access = logging_config.get("enable_access_logs", True)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ç”Ÿæˆå¸¦æ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶å
    current_date = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆå§‹åŒ–è¯¦ç»†è¯·æ±‚æ—¥å¿—
    if enable_detailed:
        request_log_filename = f"{log_dir}/requests_{current_date}.log"
        request_logger = logging.getLogger("gemini_proxy_requests")
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨ï¼Œé¿å…é‡å¤
        request_logger.handlers.clear()
        request_logger.propagate = False  # é˜²æ­¢ä¼ æ’­åˆ°æ ¹è®°å½•å™¨
        request_handler = logging.FileHandler(request_log_filename, encoding='utf-8')
        request_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        request_handler.setFormatter(request_formatter)
        request_logger.addHandler(request_handler)
        request_logger.setLevel(logging.INFO)
        logger.info(f"è¯¦ç»†è¯·æ±‚æ—¥å¿—å·²å¯ç”¨: {request_log_filename}")
    else:
        logger.info("è¯¦ç»†è¯·æ±‚æ—¥å¿—å·²ç¦ç”¨")
    
    # åˆå§‹åŒ–è®¿é—®æ—¥å¿—
    if enable_access:
        access_log_filename = f"{log_dir}/access_{current_date}.log"
        access_logger = logging.getLogger("gemini_proxy_access")
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨ï¼Œé¿å…é‡å¤
        access_logger.handlers.clear()
        access_logger.propagate = False  # é˜²æ­¢ä¼ æ’­åˆ°æ ¹è®°å½•å™¨
        access_handler = logging.FileHandler(access_log_filename, encoding='utf-8')
        access_formatter = logging.Formatter('%(asctime)s - %(message)s')
        access_handler.setFormatter(access_formatter)
        access_logger.addHandler(access_handler)
        access_logger.setLevel(logging.INFO)
        logger.info(f"è®¿é—®æ—¥å¿—å·²å¯ç”¨: {access_log_filename}")
    else:
        logger.info("è®¿é—®æ—¥å¿—å·²ç¦ç”¨")

# åŠ è½½é…ç½®
config = load_config()

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
init_logging_system(config)

# å…¨å±€é…ç½®å˜é‡
OPENAI_API_KEY = config.get("openai_api_key")
OPENAI_BASE_URL = config.get("openai_base_url", "https://api.openai.com/v1")
MODEL_MAPPING = config.get("model_mapping", {})
DEFAULT_OPENAI_MODEL = config.get("default_openai_model", "gpt-3.5-turbo")


def log_request_response(request_id: str, phase: str, data: Any, extra_info: str = "", endpoint: str = ""):
    """è®°å½•è¯·æ±‚/å“åº”çš„è¯¦ç»†æ—¥å¿—"""
    if request_logger is None:
        return  # è¯¦ç»†æ—¥å¿—å·²ç¦ç”¨
    
    log_entry = {
        "request_id": request_id,
        "timestamp": datetime.datetime.now().isoformat(),
        "endpoint": endpoint,
        "phase": phase,
        "extra_info": extra_info,
        "data": data
    }
    
    # å°†æ—¥å¿—å†™å…¥æ–‡ä»¶
    request_logger.info(f"REQUEST_LOG: {json.dumps(log_entry, ensure_ascii=False, indent=2)}")


def log_access(method: str, path: str, status_code: int, model: str = "", request_id: str = "", duration_seconds: float = 0.0):
    """è®°å½•è®¿é—®æ—¥å¿—"""
    if access_logger is None:
        return  # è®¿é—®æ—¥å¿—å·²ç¦ç”¨
    
    log_message = f"{method} {path} - {status_code} - Model: {model} - ID: {request_id[:8] if request_id else 'N/A'} - {duration_seconds:.3f}s"
    access_logger.info(log_message)
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œè®©ç”¨æˆ·çœ‹åˆ°ç³»ç»Ÿæ­£å¸¸è¿è¡Œ
    logger.info(f"ğŸš€ {log_message}")


def safe_json_serialize(obj):
    """å®‰å…¨çš„ JSON åºåˆ—åŒ–ï¼Œå¤„ç†ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡"""
    try:
        return json.loads(json.dumps(obj, default=str))
    except Exception:
        return str(obj)


@dataclass
class GeminiConfig:
    """Gemini ç”Ÿæˆé…ç½®"""
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    stop_sequences: Optional[List[str]] = None


class GeminiToOpenAIConverter:
    """Gemini æ ¼å¼åˆ° OpenAI æ ¼å¼çš„è½¬æ¢å™¨"""
    
    @staticmethod
    def convert_contents_to_messages(contents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°† Gemini contents è½¬æ¢ä¸º OpenAI messages"""
        messages = []
        
        for i, content in enumerate(contents):
            role = content.get("role", "user")
            parts = content.get("parts", [])
            
            if role == "user":
                # å¤„ç†ç”¨æˆ·æ¶ˆæ¯
                combined_text = ""
                tool_messages = []
                
                for part in parts:
                    if isinstance(part, dict):
                        if "text" in part:
                            combined_text += part["text"]
                        elif "functionResponse" in part:
                            # è½¬æ¢å‡½æ•°å“åº”ä¸ºå·¥å…·æ¶ˆæ¯
                            func_response = part["functionResponse"]
                            tool_message = {
                                "role": "tool",
                                "tool_call_id": f"{func_response.get('name', 'unknown')}:0",
                                "content": json.dumps(func_response.get("response", {}))
                            }
                            tool_messages.append(tool_message)
                    elif isinstance(part, str):
                        combined_text += part
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                if combined_text.strip():
                    messages.append({
                        "role": "user",
                        "content": combined_text.strip()
                    })
                
                # æ·»åŠ å·¥å…·æ¶ˆæ¯
                messages.extend(tool_messages)
                
            elif role == "model":
                # å¤„ç†æ¨¡å‹æ¶ˆæ¯
                combined_text = ""
                tool_calls = []
                
                for part in parts:
                    if isinstance(part, dict):
                        if "text" in part:
                            combined_text += part["text"]
                        elif "functionCall" in part:
                            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªæ¶ˆæ¯ä¸”åŒ…å« functionCall
                            is_last_message = (i == len(contents) - 1)
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ functionResponse
                            has_response = False
                            for j in range(i + 1, len(contents)):
                                next_content = contents[j]
                                if next_content.get("role") == "user":
                                    for next_part in next_content.get("parts", []):
                                        if isinstance(next_part, dict) and "functionResponse" in next_part:
                                            # æ£€æŸ¥ functionResponse çš„åç§°æ˜¯å¦åŒ¹é…
                                            func_name = part["functionCall"].get("name")
                                            resp_name = next_part["functionResponse"].get("name")
                                            if func_name == resp_name:
                                                has_response = True
                                                break
                                if has_response:
                                    break
                            
                            # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ¶ˆæ¯ä¸”æ²¡æœ‰å¯¹åº”çš„å“åº”ï¼Œè·³è¿‡è¿™ä¸ª functionCall
                            # è¿™é¿å…äº† "tool_calls must be followed by tool messages" é”™è¯¯
                            if is_last_message and not has_response:
                                continue
                            
                            # è½¬æ¢å‡½æ•°è°ƒç”¨ä¸ºå·¥å…·è°ƒç”¨
                            func_call = part["functionCall"]
                            # ä½¿ç”¨æ­£ç¡®çš„ ID æ ¼å¼ï¼šfunction_name:index
                            tool_call = {
                                "id": f"{func_call.get('name', 'unknown')}:0",
                                "type": "function",
                                "function": {
                                    "name": func_call.get("name", ""),
                                    "arguments": json.dumps(func_call.get("args", {}))
                                }
                            }
                            tool_calls.append(tool_call)
                    elif isinstance(part, str):
                        combined_text += part
                
                # åªæœ‰åœ¨æœ‰å†…å®¹æˆ–å·¥å…·è°ƒç”¨æ—¶æ‰æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
                if combined_text.strip() or tool_calls:
                    assistant_message = {
                        "role": "assistant",
                        "content": combined_text.strip() if combined_text.strip() else None
                    }
                    if tool_calls:
                        assistant_message["tool_calls"] = tool_calls
                        
                    messages.append(assistant_message)
        
        return messages
    
    @staticmethod
    def convert_config_to_openai_params(config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """å°† Gemini é…ç½®è½¬æ¢ä¸º OpenAI å‚æ•°"""
        if not config:
            return {}
        
        openai_params = {}
        
        # å‚æ•°æ˜ å°„
        if "temperature" in config:
            openai_params["temperature"] = config["temperature"]
        if "maxOutputTokens" in config:
            openai_params["max_tokens"] = config["maxOutputTokens"]
        if "topP" in config:
            openai_params["top_p"] = config["topP"]
        if "stopSequences" in config:
            openai_params["stop"] = config["stopSequences"]
            
        return openai_params
    
    @staticmethod
    def convert_tools_to_openai(tools: Optional[List[Dict[str, Any]]]) -> Optional[List[Dict[str, Any]]]:
        """è½¬æ¢å·¥å…·å®šä¹‰"""
        if not tools:
            return None
        
        openai_tools = []
        for tool in tools:
            if "functionDeclarations" in tool:
                for func_decl in tool["functionDeclarations"]:
                    openai_tool = {
                        "type": "function",
                        "function": {
                            "name": func_decl.get("name", ""),
                            "description": func_decl.get("description", ""),
                            "parameters": func_decl.get("parameters", {})
                        }
                    }
                    openai_tools.append(openai_tool)
        
        return openai_tools if openai_tools else None


class OpenAIToGeminiConverter:
    """OpenAI æ ¼å¼åˆ° Gemini æ ¼å¼çš„è½¬æ¢å™¨"""
    
    @staticmethod
    def clean_markdown_json(content: str) -> str:
        """æ¸…ç† markdown æ ¼å¼çš„ JSON ä»£ç å—ï¼Œæå–çº¯ JSON å†…å®¹"""
        import re
        
        # åŒ¹é… ```json\n...å†…å®¹...\n``` æ ¼å¼
        json_pattern = r'^```json\s*\n(.*?)\n```$'
        match = re.match(json_pattern, content.strip(), re.DOTALL)
        
        if match:
            # æå– JSON å†…å®¹
            json_content = match.group(1)
            return json_content
        
        # å¦‚æœä¸åŒ¹é…æ¨¡å¼ï¼Œè¿”å›åŸå†…å®¹
        return content
    
    @staticmethod
    def convert_response_to_gemini(response: Any) -> Dict[str, Any]:
        """å°† OpenAI å“åº”è½¬æ¢ä¸º Gemini æ ¼å¼"""
        choice = response.choices[0]
        message = choice.message
        
        # æ„å»º parts
        parts = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if message.content:
            # æ¸…ç† markdown æ ¼å¼çš„ JSON ä»£ç å—
            cleaned_content = OpenAIToGeminiConverter.clean_markdown_json(message.content)
            parts.append({"text": cleaned_content})
        
        # æ·»åŠ å·¥å…·è°ƒç”¨
        if hasattr(message, 'tool_calls') and message.tool_calls:
            for tool_call in message.tool_calls:
                function_call_part = {
                    "functionCall": {
                        "name": tool_call.function.name,
                        "args": json.loads(tool_call.function.arguments)
                    }
                }
                parts.append(function_call_part)
        
        # æ˜ å°„å®ŒæˆåŸå› 
        finish_reason_mapping = {
            "stop": "STOP",
            "length": "MAX_TOKENS",
            "content_filter": "SAFETY",
            "tool_calls": "STOP",
            "function_call": "STOP"
        }
        finish_reason = finish_reason_mapping.get(choice.finish_reason, "STOP")
        
        # æ„å»ºå“åº”
        gemini_response = {
            "candidates": [{
                "content": {
                    "parts": parts,
                    "role": "model"
                },
                "finishReason": finish_reason,
                "index": 0,
                "safetyRatings": None
            }],
            "promptFeedback": {
                "safetyRatings": None
            }
        }
        
        # æ·»åŠ ä½¿ç”¨ä¿¡æ¯
        if hasattr(response, 'usage') and response.usage:
            usage_metadata = {
                "promptTokenCount": response.usage.prompt_tokens,
                "candidatesTokenCount": response.usage.completion_tokens,
                "totalTokenCount": response.usage.total_tokens
            }
            
            # æ·»åŠ è¯¦ç»†çš„ token ä¿¡æ¯
            if hasattr(response.usage, 'prompt_tokens_details'):
                usage_metadata["promptTokensDetails"] = [{
                    "modality": "TEXT",
                    "tokenCount": response.usage.prompt_tokens
                }]
            else:
                # å¦‚æœæ²¡æœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
                usage_metadata["promptTokensDetails"] = [{
                    "modality": "TEXT", 
                    "tokenCount": response.usage.prompt_tokens
                }]
            
            # æ·»åŠ æ€è€ƒ token è®¡æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œè¿™æ˜¯ Gemini ç‰¹æœ‰çš„ï¼‰
            # OpenAI æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œæˆ‘ä»¬æš‚æ—¶è®¾ä¸º 0 æˆ–ä¸åŒ…å«
            # usage_metadata["thoughtsTokenCount"] = 0
            
            gemini_response["usageMetadata"] = usage_metadata
        
        return gemini_response
    
    @staticmethod
    def convert_streaming_chunk_to_gemini(chunk: Any, accumulated_tool_calls: Dict) -> Optional[Dict[str, Any]]:
        """è½¬æ¢æµå¼å“åº”å—ä¸º Gemini æ ¼å¼"""
        if not chunk.choices:
            return None
            
        choice = chunk.choices[0]
        if not choice.delta:
            return None
        
        delta = choice.delta
        parts = []
        
        # å¤„ç†æ–‡æœ¬å†…å®¹
        if delta.content:
            # å¯¹äºæµå¼å“åº”ï¼Œæš‚æ—¶ä¸å¤„ç† markdown æ¸…ç†ï¼Œå› ä¸ºå†…å®¹æ˜¯åˆ†å—çš„
            # markdown æ¸…ç†å°†åœ¨æœ€ç»ˆåˆå¹¶æ—¶å¤„ç†
            parts.append({"text": delta.content})
        
        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆéœ€è¦ç´¯ç§¯ï¼‰
        if hasattr(delta, 'tool_calls') and delta.tool_calls:
            for tool_call in delta.tool_calls:
                # ä½¿ç”¨ index ä½œä¸ºä¸»é”®ï¼Œå› ä¸ºæµå¼å“åº”ä¸­ id ç»å¸¸ä¸º None
                tool_call_index = tool_call.index if tool_call.index is not None else 0
                tool_call_key = f"tool_{tool_call_index}"
                
                # åˆå§‹åŒ–ç´¯ç§¯çŠ¶æ€
                if tool_call_key not in accumulated_tool_calls:
                    accumulated_tool_calls[tool_call_key] = {
                        "id": "",
                        "name": "",
                        "arguments": ""
                    }
                
                # ç´¯ç§¯å·¥å…·è°ƒç”¨ IDï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å‡ºç°æ—¶è®¾ç½®ï¼‰
                if tool_call.id and not accumulated_tool_calls[tool_call_key]["id"]:
                    accumulated_tool_calls[tool_call_key]["id"] = tool_call.id
                
                # ç´¯ç§¯å‡½æ•°åï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å‡ºç°æ—¶è®¾ç½®ï¼‰
                if tool_call.function and tool_call.function.name and not accumulated_tool_calls[tool_call_key]["name"]:
                    accumulated_tool_calls[tool_call_key]["name"] = tool_call.function.name
                
                # ç´¯ç§¯å‡½æ•°å‚æ•°
                if tool_call.function and tool_call.function.arguments:
                    accumulated_tool_calls[tool_call_key]["arguments"] += tool_call.function.arguments
                
                # å°è¯•è§£æå®Œæ•´çš„ JSON
                try:
                    args_str = accumulated_tool_calls[tool_call_key]["arguments"]
                    name = accumulated_tool_calls[tool_call_key]["name"]
                    
                    if args_str and name:
                        parsed_args = json.loads(args_str)
                        # JSON å®Œæ•´ï¼Œåˆ›å»ºå‡½æ•°è°ƒç”¨éƒ¨åˆ†
                        function_call_part = {
                            "functionCall": {
                                "name": name,
                                "args": parsed_args
                            }
                        }
                        parts.append(function_call_part)
                        # æ¸…ç†å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
                        del accumulated_tool_calls[tool_call_key]
                except json.JSONDecodeError:
                    # JSON ä¸å®Œæ•´ï¼Œç»§ç»­ç´¯ç§¯
                    pass
        
        if not parts and not choice.finish_reason:
            return None
        
        # æ˜ å°„å®ŒæˆåŸå› 
        finish_reason_mapping = {
            "stop": "STOP",
            "length": "MAX_TOKENS", 
            "content_filter": "SAFETY",
            "tool_calls": "STOP"
        }
        
        return {
            "candidates": [{
                "content": {"parts": parts, "role": "model"},
                "finishReason": finish_reason_mapping.get(choice.finish_reason) if choice.finish_reason else None,
                "index": 0,
                "safetyRatings": []
            }]
        }


class GeminiProxyService:
    """Gemini API ä»£ç†æœåŠ¡"""
    
    def __init__(self, openai_api_key: str, openai_base_url: str = "https://api.openai.com/v1"):
        self.client = AsyncOpenAI(
            api_key=openai_api_key,
            base_url=openai_base_url
        )
        self.converter_to_openai = GeminiToOpenAIConverter()
        self.converter_to_gemini = OpenAIToGeminiConverter()
    
    def map_gemini_model_to_openai(self, gemini_model: str) -> str:
        """å°† Gemini æ¨¡å‹åç§°æ˜ å°„åˆ° OpenAI æ¨¡å‹åç§°"""
        mapped_model = MODEL_MAPPING.get(gemini_model, DEFAULT_OPENAI_MODEL)
        logger.info(f"æ¨¡å‹æ˜ å°„: {gemini_model} -> {mapped_model}")
        return mapped_model
    
    async def generate_content(self, request_data: Dict[str, Any], request_id: str = None, endpoint: str = "") -> Dict[str, Any]:
        """å¤„ç† generateContent è¯·æ±‚"""
        if request_id is None:
            request_id = str(uuid.uuid4())
            
        try:
            # è®°å½•åŸå§‹ Gemini è¯·æ±‚
            log_request_response(
                request_id, 
                "1_GEMINI_REQUEST", 
                safe_json_serialize(request_data), 
                "ç”¨æˆ·å‘é€çš„ Gemini æ ¼å¼è¯·æ±‚",
                endpoint
            )
            
            # æå–å‚æ•°
            gemini_model = request_data.get("model", "gemini-1.5-pro")
            contents = request_data.get("contents", [])
            system_instruction = request_data.get("systemInstruction")
            generation_config = request_data.get("generationConfig")
            tools = request_data.get("tools")
            
            # æ˜ å°„ Gemini æ¨¡å‹åˆ° OpenAI æ¨¡å‹
            openai_model = self.map_gemini_model_to_openai(gemini_model)
            
            # è½¬æ¢ä¸º OpenAI æ ¼å¼
            messages = self.converter_to_openai.convert_contents_to_messages(contents)
            
            # å¤„ç†ç³»ç»ŸæŒ‡ä»¤ï¼šå¦‚æœæœ‰ systemInstructionï¼Œæ·»åŠ ä¸ºç¬¬ä¸€ä¸ª system message
            if system_instruction and system_instruction.get("parts"):
                system_text = ""
                for part in system_instruction["parts"]:
                    if "text" in part:
                        system_text += part["text"]
                
                if system_text.strip():
                    # åœ¨æ¶ˆæ¯åˆ—è¡¨å¼€å¤´æ’å…¥ system message
                    messages.insert(0, {
                        "role": "system",
                        "content": system_text.strip()
                    })
            openai_params = self.converter_to_openai.convert_config_to_openai_params(generation_config)
            openai_tools = self.converter_to_openai.convert_tools_to_openai(tools)
            
            # æ„å»º OpenAI è¯·æ±‚
            completion_params = {
                "model": openai_model,  # ä½¿ç”¨æ˜ å°„åçš„æ¨¡å‹
                "messages": messages,
                **openai_params
            }
            
            if openai_tools:
                completion_params["tools"] = openai_tools
                completion_params["tool_choice"] = "auto"
            
            # è®°å½•è½¬æ¢åçš„ OpenAI è¯·æ±‚
            log_request_response(
                request_id,
                "2_OPENAI_REQUEST", 
                safe_json_serialize(completion_params),
                f"è½¬æ¢ä¸º OpenAI æ ¼å¼çš„è¯·æ±‚ï¼Œæ¨¡å‹æ˜ å°„: {gemini_model} -> {openai_model}",
                endpoint
            )
            
            # è°ƒç”¨ OpenAI API
            response = await self.client.chat.completions.create(**completion_params)
            
            # è®°å½• OpenAI åŸå§‹å“åº”
            log_request_response(
                request_id,
                "3_OPENAI_RESPONSE",
                safe_json_serialize(response),
                "OpenAI API è¿”å›çš„åŸå§‹å“åº”",
                endpoint
            )
            
            # è½¬æ¢å“åº”ä¸º Gemini æ ¼å¼
            gemini_response = self.converter_to_gemini.convert_response_to_gemini(response)
            
            # è®°å½•æœ€ç»ˆçš„ Gemini å“åº”
            log_request_response(
                request_id,
                "4_GEMINI_RESPONSE",
                safe_json_serialize(gemini_response),
                "è¿”å›ç»™ç”¨æˆ·çš„ Gemini æ ¼å¼å“åº”",
                endpoint
            )
            
            return gemini_response
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            log_request_response(
                request_id,
                "ERROR",
                {"error": str(e), "type": type(e).__name__},
                "è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                endpoint
            )
            logger.error(f"Error in generate_content: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    async def stream_generate_content(self, request_data: Dict[str, Any], request_id: str = None, endpoint: str = "") -> AsyncGenerator[str, None]:
        """å¤„ç†æµå¼ generateContent è¯·æ±‚"""
        if request_id is None:
            request_id = str(uuid.uuid4())
            
        try:
            # è®°å½•åŸå§‹ Gemini æµå¼è¯·æ±‚
            log_request_response(
                request_id, 
                "1_GEMINI_STREAM_REQUEST", 
                safe_json_serialize(request_data), 
                "ç”¨æˆ·å‘é€çš„ Gemini æ ¼å¼æµå¼è¯·æ±‚",
                endpoint
            )
            
            # æå–å‚æ•°
            gemini_model = request_data.get("model", "gemini-1.5-pro")
            contents = request_data.get("contents", [])
            system_instruction = request_data.get("systemInstruction")
            generation_config = request_data.get("generationConfig")
            tools = request_data.get("tools")
            
            # æ˜ å°„ Gemini æ¨¡å‹åˆ° OpenAI æ¨¡å‹
            openai_model = self.map_gemini_model_to_openai(gemini_model)
            
            # è½¬æ¢ä¸º OpenAI æ ¼å¼
            messages = self.converter_to_openai.convert_contents_to_messages(contents)
            
            # å¤„ç†ç³»ç»ŸæŒ‡ä»¤ï¼šå¦‚æœæœ‰ systemInstructionï¼Œæ·»åŠ ä¸ºç¬¬ä¸€ä¸ª system message
            if system_instruction and system_instruction.get("parts"):
                system_text = ""
                for part in system_instruction["parts"]:
                    if "text" in part:
                        system_text += part["text"]
                
                if system_text.strip():
                    # åœ¨æ¶ˆæ¯åˆ—è¡¨å¼€å¤´æ’å…¥ system message
                    messages.insert(0, {
                        "role": "system",
                        "content": system_text.strip()
                    })
            openai_params = self.converter_to_openai.convert_config_to_openai_params(generation_config)
            openai_tools = self.converter_to_openai.convert_tools_to_openai(tools)
            
            # æ„å»º OpenAI è¯·æ±‚
            completion_params = {
                "model": openai_model,  # ä½¿ç”¨æ˜ å°„åçš„æ¨¡å‹
                "messages": messages,
                "stream": True,
                **openai_params
            }
            
            if openai_tools:
                completion_params["tools"] = openai_tools
                completion_params["tool_choice"] = "auto"
            
            # è®°å½•è½¬æ¢åçš„ OpenAI æµå¼è¯·æ±‚
            log_request_response(
                request_id,
                "2_OPENAI_STREAM_REQUEST", 
                safe_json_serialize(completion_params),
                f"è½¬æ¢ä¸º OpenAI æ ¼å¼çš„æµå¼è¯·æ±‚ï¼Œæ¨¡å‹æ˜ å°„: {gemini_model} -> {openai_model}",
                endpoint
            )
            
            # è°ƒç”¨ OpenAI æµå¼ API
            stream = await self.client.chat.completions.create(**completion_params)
            
            # ç´¯ç§¯å·¥å…·è°ƒç”¨çŠ¶æ€å’Œå“åº”å†…å®¹
            accumulated_tool_calls = {}
            all_chunks = []  # è®°å½•æ‰€æœ‰æµå¼å—
            
            async for chunk in stream:
                # è®°å½• OpenAI æµå¼å—ï¼ˆé‡‡æ ·è®°å½•ï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                if len(all_chunks) < 5 or len(all_chunks) % 10 == 0:
                    log_request_response(
                        request_id,
                        f"3_OPENAI_STREAM_CHUNK_{len(all_chunks)}",
                        safe_json_serialize(chunk),
                        f"OpenAI æµå¼å“åº”å— #{len(all_chunks)}",
                        endpoint
                    )
                
                all_chunks.append(chunk)
                
                gemini_chunk = self.converter_to_gemini.convert_streaming_chunk_to_gemini(
                    chunk, accumulated_tool_calls
                )
                
                if gemini_chunk:
                    # è®°å½•è½¬æ¢åçš„ Gemini æµå¼å—ï¼ˆé‡‡æ ·è®°å½•ï¼‰
                    if len(all_chunks) < 5 or len(all_chunks) % 10 == 0:
                        log_request_response(
                            request_id,
                            f"4_GEMINI_STREAM_CHUNK_{len(all_chunks)}",
                            safe_json_serialize(gemini_chunk),
                            f"è½¬æ¢åçš„ Gemini æµå¼å“åº”å— #{len(all_chunks)}",
                            endpoint
                        )
                    
                    chunk_data = json.dumps(gemini_chunk, ensure_ascii=False)
                    yield f"data: {chunk_data}\n\n"
            
            # å¤„ç†æµå¼ç»“æŸæ—¶å‰©ä½™çš„å·¥å…·è°ƒç”¨
            if accumulated_tool_calls:
                for tool_data in accumulated_tool_calls.values():
                    name = tool_data.get("name")
                    args_str = tool_data.get("arguments")
                    
                    if name and args_str:
                        try:
                            parsed_args = json.loads(args_str)
                            # åˆ›å»ºæœ€ç»ˆçš„å‡½æ•°è°ƒç”¨å“åº”
                            final_gemini_chunk = {
                                "candidates": [{
                                    "content": {
                                        "parts": [{
                                            "functionCall": {
                                                "name": name,
                                                "args": parsed_args
                                            }
                                        }],
                                        "role": "model"
                                    },
                                    "finishReason": "STOP",
                                    "index": 0,
                                    "safetyRatings": []
                                }]
                            }
                            
                            # è®°å½•æœ€ç»ˆçš„å·¥å…·è°ƒç”¨å—
                            log_request_response(
                                request_id,
                                f"4_GEMINI_STREAM_CHUNK_FINAL",
                                safe_json_serialize(final_gemini_chunk),
                                f"æœ€ç»ˆçš„ Gemini å·¥å…·è°ƒç”¨å“åº”å—",
                                endpoint
                            )
                            
                            chunk_data = json.dumps(final_gemini_chunk, ensure_ascii=False)
                            yield f"data: {chunk_data}\n\n"
                        except json.JSONDecodeError:
                            # å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè®°å½•ä½†ç»§ç»­
                            log_request_response(
                                request_id,
                                "ERROR",
                                {"tool_data": tool_data, "error": "JSON è§£æå¤±è´¥"},
                                "å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥",
                                endpoint
                            )

            # è®°å½•å®Œæ•´çš„æµå¼å“åº”æ€»ç»“
            log_request_response(
                request_id,
                "5_STREAM_SUMMARY",
                {"total_chunks": len(all_chunks), "accumulated_tool_calls": accumulated_tool_calls},
                f"æµå¼å“åº”å®Œæˆï¼Œæ€»å…± {len(all_chunks)} ä¸ªå—",
                endpoint
            )
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            log_request_response(
                request_id,
                "STREAM_ERROR",
                {"error": str(e), "type": type(e).__name__},
                "æµå¼è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                endpoint
            )
            logger.error(f"Error in stream_generate_content: {e}")
            error_response = {
                "error": {
                    "message": str(e),
                    "type": "internal_error"
                }
            }
            yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="Gemini API Proxy", version="1.0.0")

# åˆå§‹åŒ–ä»£ç†æœåŠ¡
proxy_service = GeminiProxyService(
    openai_api_key=OPENAI_API_KEY,
    openai_base_url=OPENAI_BASE_URL
)


async def _generate_content_internal(model_name: str, request: Request, log_suffix: str = ""):
    """å†…éƒ¨ generateContent é€»è¾‘ï¼Œé¿å…é‡å¤è®°å½•æ—¥å¿—"""
    start_time = datetime.datetime.now()
    request_id = str(uuid.uuid4())
    
    try:
        request_data = await request.json()
        
        # è·å–è¯·æ±‚çš„ç«¯ç‚¹è·¯å¾„
        endpoint = f"{request.method} {request.url.path}"
        
        # å¦‚æœè¯·æ±‚ä¸­æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨è·¯å¾„ä¸­çš„æ¨¡å‹
        if "model" not in request_data:
            request_data["model"] = model_name
        
        result = await proxy_service.generate_content(request_data, request_id, endpoint)
        
        # è®°å½•æˆåŠŸè®¿é—®æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        model_display = f"{model_name}{log_suffix}" if log_suffix else model_name
        log_access("POST", request.url.path, 200, model_display, request_id, duration)
        
        return result
        
    except Exception as e:
        # è®°å½•å¤±è´¥è®¿é—®æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        model_display = f"{model_name}{log_suffix}" if log_suffix else model_name
        log_access("POST", request.url.path, 500, model_display, request_id, duration)
        raise


@app.post("/v1beta/models/{model_name}:generateContent")
@app.post("/models/{model_name}:generateContent")
async def generate_content(model_name: str, request: Request):
    """Gemini generateContent ç«¯ç‚¹"""
    return await _generate_content_internal(model_name, request)


async def _stream_with_logging(generator, start_time, path, model_name, request_id):
    """åŒ…è£…æµå¼ç”Ÿæˆå™¨ï¼Œåœ¨å®Œæˆæ—¶è®°å½•æ—¥å¿—"""
    try:
        async for chunk in generator:
            yield chunk
        # æµå¼å®Œæˆï¼Œè®°å½•æˆåŠŸæ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        log_access("POST", path, 200, f"{model_name}(stream)", request_id, duration)
    except Exception as e:
        # æµå¼å¤±è´¥ï¼Œè®°å½•å¤±è´¥æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        log_access("POST", path, 500, f"{model_name}(stream)", request_id, duration)
        raise


@app.post("/v1beta/models/{model_name}:streamGenerateContent")  
@app.post("/models/{model_name}:streamGenerateContent")
async def stream_generate_content(model_name: str, request: Request):
    """Gemini æµå¼ generateContent ç«¯ç‚¹"""
    start_time = datetime.datetime.now()
    request_id = str(uuid.uuid4())
    
    try:
        request_data = await request.json()
        
        # è·å–è¯·æ±‚çš„ç«¯ç‚¹è·¯å¾„
        endpoint = f"{request.method} {request.url.path}"
        
        # å¦‚æœè¯·æ±‚ä¸­æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨è·¯å¾„ä¸­çš„æ¨¡å‹
        if "model" not in request_data:
            request_data["model"] = model_name
        
        # åˆ›å»ºå¸¦æ—¥å¿—è®°å½•çš„æµå¼ç”Ÿæˆå™¨
        stream_generator = _stream_with_logging(
            proxy_service.stream_generate_content(request_data, request_id, endpoint),
            start_time,
            request.url.path,
            model_name,
            request_id
        )
        
        return StreamingResponse(
            stream_generator,
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
            }
        )
        
    except Exception as e:
        # è®°å½•å¤±è´¥è®¿é—®æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        log_access("POST", request.url.path, 500, f"{model_name}(stream)", request_id, duration)
        raise


@app.post("/v1beta/models/{model_name}:countTokens")
@app.post("/models/{model_name}:countTokens")
async def count_tokens(model_name: str, request: Request):
    """Gemini countTokens ç«¯ç‚¹ - å®Œå…¨ç­‰åŒäº generateContent"""
    # ç›´æ¥è°ƒç”¨å†…éƒ¨é€»è¾‘ï¼Œé¿å…é‡å¤æ—¥å¿—è®°å½•
    return await _generate_content_internal(model_name, request, "(count)")


@app.get("/health")
async def health_check(request: Request):
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    start_time = datetime.datetime.now()
    
    try:
        result = {"status": "healthy", "service": "gemini-proxy"}
        
        # è®°å½•æˆåŠŸè®¿é—®æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        log_access("GET", request.url.path, 200, "health", "", duration)
        
        return result
        
    except Exception as e:
        # è®°å½•å¤±è´¥è®¿é—®æ—¥å¿—
        duration = (datetime.datetime.now() - start_time).total_seconds()
        log_access("GET", request.url.path, 500, "health", "", duration)
        raise


if __name__ == "__main__":
    # ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    runtime_config = load_config()
    
    # é‡æ–°åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆæ”¯æŒè¿è¡Œæ—¶é…ç½®ï¼‰
    init_logging_system(runtime_config)
    
    api_key = runtime_config.get("openai_api_key")
    base_url = runtime_config.get("openai_base_url")
    server_config = runtime_config.get("server", {})
    logging_config = runtime_config.get("logging", {})
    
    # è·å–æœåŠ¡å™¨é…ç½®
    host = server_config.get("host", "0.0.0.0")
    port = server_config.get("port", 8000)
    log_level = server_config.get("log_level", "info")
    
    # æ›´æ–°å…¨å±€é…ç½®
    proxy_service = GeminiProxyService(
        openai_api_key=api_key,
        openai_base_url=base_url
    )
    
    logger.info(f"ä½¿ç”¨é…ç½® - API Key: {api_key[:10] if api_key else 'None'}..., Base URL: {base_url}")
    logger.info(f"æœåŠ¡å™¨é…ç½® - Host: {host}, Port: {port}, Log Level: {log_level}")
    logger.info(f"æ—¥å¿—é…ç½® - è¯¦ç»†æ—¥å¿—: {logging_config.get('enable_detailed_logs', False)}, è®¿é—®æ—¥å¿—: {logging_config.get('enable_access_logs', True)}")
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level=log_level
    )